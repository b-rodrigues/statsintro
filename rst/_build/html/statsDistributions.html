

<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>Characterizing a Distribution &mdash; Introduction to Statistics 1.0 documentation</title>
    
    <link rel="stylesheet" href="_static/default.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '',
        VERSION:     '1.0',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="_static/jquery.js"></script>
    <script type="text/javascript" src="_static/underscore.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="top" title="Introduction to Statistics 1.0 documentation" href="index.html" />
    <link rel="next" title="Hypothesis tests" href="statsTests.html" />
    <link rel="prev" title="Datatypes" href="statsBasics.html" /> 
  </head>
  <body>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="statsTests.html" title="Hypothesis tests"
             accesskey="N">next</a> |</li>
        <li class="right" >
          <a href="statsBasics.html" title="Datatypes"
             accesskey="P">previous</a> |</li>
        <li><a href="StatsFH.html">Introduction to Statistics 1.0 documentation</a> &raquo;</li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body">
            
  <img alt="..\Images\title_distribution.png" src="..\Images\title_distribution.png" style="height: 100px;" />
<div class="section" id="characterizing-a-distribution">
<h1>Characterizing a Distribution<a class="headerlink" href="#characterizing-a-distribution" title="Permalink to this headline">¶</a></h1>
<div class="section" id="distribution-center">
<h2>Distribution Center<a class="headerlink" href="#distribution-center" title="Permalink to this headline">¶</a></h2>
<div class="section" id="mean">
<h3>Mean<a class="headerlink" href="#mean" title="Permalink to this headline">¶</a></h3>
<p>By default, when we talk about the <em>mean value</em> we mean the <em>arithmetic
mean</em> <span class="math">\(\bar{x}\)</span>:</p>
<div class="math">
\[\bar{x} = \frac{{\sum\limits_{i = 1}^n {{x_i}} }}{n}\]</div>
</div>
<div class="section" id="median">
<h3>Median<a class="headerlink" href="#median" title="Permalink to this headline">¶</a></h3>
<p>The <em>median</em> is that value that comes half-way when the data are ranked
in order. In contrast to the mean, it is not affected by outlying data
points.</p>
</div>
<div class="section" id="mode">
<h3>Mode<a class="headerlink" href="#mode" title="Permalink to this headline">¶</a></h3>
<p>The <em>mode</em> value is the most frequently occurring value in a
distribution.</p>
</div>
<div class="section" id="geometric-mean">
<h3>Geometric Mean<a class="headerlink" href="#geometric-mean" title="Permalink to this headline">¶</a></h3>
<p>In some situations the <em>geometric mean</em> can be useful to describe the
location of a distribution. It is usually close to the median, and can
be calculated via the arithmetic mean of the log of the values.</p>
</div>
</div>
<div class="section" id="quantifying-variability">
<h2>Quantifying Variability<a class="headerlink" href="#quantifying-variability" title="Permalink to this headline">¶</a></h2>
<div class="section" id="range">
<h3>Range<a class="headerlink" href="#range" title="Permalink to this headline">¶</a></h3>
<p>This one is fairly easy: it is the difference between the highest and
the lowest data value. The only thing that you have to watch out for:
after you have acquired your data, you have to check for <em>outliers</em>,
i.e. data points with a value much higher or lower than the rest of the
data. Often, such points are caused by errors in the selection of the
sample or in the measurement procedure. There are a number of tests to
check for outliers. One of them is to check for data which lie more than
1.5*<em>inter-quartile-range</em> (IQR) above or below the first/third
quartile (see below).</p>
</div>
<div class="section" id="centiles">
<h3>Centiles<a class="headerlink" href="#centiles" title="Permalink to this headline">¶</a></h3>
<p>The <em>Cumulative distribution function (CDF) * tells you for each value
which percentage of the data has a lower value (Figure [fig:CDF]). The
value below which a given percentage of the values occur is called
*centile</em> or <em>percentile</em>, and corresponds to a value with a specified
cumulative frequency.</p>
<p>For example, when you look for the data range which includes 95% of the
data, you have to find the <span class="math">\(2.5^{th}\)</span> and the <span class="math">\(97.5^{th}\)</span>
percentile of your sample distribution.</p>
<p>The 50th percentile is the <em>median</em>.</p>
<p>Also important are the <em>quartiles</em>, i.e. the 25th and the 75th
percentile. The difference between them is sometimes referred to as
<em>inter-quartile range (IQR)</em>.</p>
<p>Median, upper and lower quartile are used for the data display in box
plots.</p>
<div class="line-block">
<div class="line"><a href="#id2"><span class="problematic" id="id3">|i mage7|</span></a></div>
</div>
<p><em>Probability distribution function (top) and Cumulative distribution function (bot-
tom) of a normal distribution.</em></p>
</div>
<div class="section" id="standard-deviation-and-variance">
<h3>Standard Deviation and Variance<a class="headerlink" href="#standard-deviation-and-variance" title="Permalink to this headline">¶</a></h3>
<p>The <em>variance</em> (SD) of a distribution is defined as</p>
<div class="math">
\[var = \frac{{\sum\limits_{i = 1}^n {({x_i-\bar{x}})^2} }}{n-1}\]</div>
<p>Note that we divide by <em>n-1</em> rather than the more obvious n: dividing by
<span class="math">\(n\)</span> gives the variance of the observations around the sample mean,
but we virtually always consider our data as a sample from some larger
population and wish to use the sample data to estimate the variability
in the population. Dividing by <span class="math">\(n-1\)</span> gives us a better estimate of
the population variance.</p>
<p>The <em>standard deviation</em> is simply given by the square root of the
variance:</p>
<div class="math">
\[s = \sqrt{var}\]</div>
<p>In statistics it is often common to denote the population standard
deviation with <span class="math">\(\sigma\)</span>, and the sample standard deviation with
<span class="math">\(s\)</span>.</p>
<p>Watch out: in Python, by default the variance is calculated for &#8220;n&#8221;. You
have to set &#8220;ddof=1&#8221; to obtain the variance for &#8220;n-1&#8221;:</p>
<div class="highlight-python"><pre>In[19]: data = arange(7,14)

In[20]: std(data, ddof=0)
Out[20]: 2.0

In[21]: std(data, ddof=1)
Out[21]: 2.1602468994692865</pre>
</div>
</div>
<div class="section" id="standard-error">
<h3>Standard Error<a class="headerlink" href="#standard-error" title="Permalink to this headline">¶</a></h3>
<p>While the standard deviation is a good measure for the distribution of
your values, often you are more interested in the distribution of the
mean value. For example, when you measure the response to a new
medication, you might be interested in how well you can characterize
this response, i.e. is how well you know the mean value. This measure is
called the <em>standard error of the mean</em>, or sometimes just the <em>standard
error</em>. In a single sample from a population with a standard deviation
of <span class="math">\(\sigma\)</span> the variance of the sampling distribution of the mean
is <span class="math">\(\sigma^2/n\)</span>, and so the standard error of the mean is
<span class="math">\(\sigma/\sqrt{n}\)</span>.</p>
</div>
<div class="section" id="skewness">
<h3>Skewness<a class="headerlink" href="#skewness" title="Permalink to this headline">¶</a></h3>
<p>Distributions are <em>skewed</em> if they depart from symmetry. For example, if
you have a measurement that cannot be negative, which is usually the
case, then we can infer that the data have a skewed distribution if the
standard deviation is more than half the mean. Such an asymmetry is
referred to as <em>positive skewness</em>. The opposite, negative skewness, is
rare.</p>
</div>
<div class="section" id="central-limit-theorem">
<h3>Central Limit Theorem<a class="headerlink" href="#central-limit-theorem" title="Permalink to this headline">¶</a></h3>
<p>The central limit theorem states that for identically distributed
independent random variables (also referred to as <em>random variates</em>),
the mean of a sufficiently large number of these variables will be
approximately normally distributed.</p>
</div>
</div>
</div>
<div class="section" id="distribution-functions">
<h1>Distribution Functions<a class="headerlink" href="#distribution-functions" title="Permalink to this headline">¶</a></h1>
<p>The variable for a standardized distribution function is often called
<em>statistic</em>. So you often find expressions like &#8220;the z-statistic&#8221; (for
the normal distribution function), the &#8220;t-statistic&#8221; (for the
t-distribution) or the &#8220;F-statistic&#8221; (for the F-distribution).</p>
<div class="section" id="probability-and-samples">
<h2>Probability and Samples<a class="headerlink" href="#probability-and-samples" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="normal-distribution">
<h2>Normal Distribution<a class="headerlink" href="#normal-distribution" title="Permalink to this headline">¶</a></h2>
<p>The <em>Normal distribution</em> or <em>Gaussian distribution</em> is by far the most
important of all the distribution functions. This is due to the fact
that the mean values of <em>all</em> distribution functions approximate a
normal distribution for large enough sample numbers. Mathematically, the
normal distribution is characterized by a mean value <span class="math">\(\mu\)</span>, and a
standard deviation <span class="math">\(\sigma\)</span>:</p>
<div class="math">
\[\label{eq_normal}
     f_{\mu,\sigma} (x) = \frac{1}{\sigma \sqrt{2 \pi}} e^{-( x - \mu )^2 /2 \sigma^2}\]</div>
<p>where :math:` - infty &lt; x &lt; infty <cite>, and :math:`f_{mu,sigma}</cite> is the
<em>probability density function (PDF)</em> .</p>
<div class="line-block">
<div class="line"><a class="reference internal" href="_images/Normal_Distribution_PDF.png"><img alt="image8" src="_images/Normal_Distribution_PDF.png" style="width: 500.0px; height: 319.5px;" /></a></div>
</div>
<p><em>Normal Distribution</em></p>
<p>For smaller sample numbers, the sample distribution can show quite a bit
of variability. For example, look at 25 distributions generated by
sampling 100 numbers from a normal distribution:</p>
<div class="line-block">
<div class="line"><a class="reference internal" href="_images/Normal_MultHist.png"><img alt="image9" src="_images/Normal_MultHist.png" style="width: 407.5px; height: 307.5px;" /></a></div>
</div>
<p><em>25 randomly generated normal distributions of 100 points.</em></p>
<p>Some examples of applications are:</p>
<ul class="simple">
<li>If the average man is 175 cm tall with a variance of 6 cm, what is
the probability that a man found at random will be 183 cm tall?</li>
<li>If the average man is 175 cm tall with a variance of 6 cm and the
average woman is 168 cm tall with a variance of 3 cm, what is the
probability that the average man from a given sample will be shorter
than the average woman from a given sample?</li>
<li>If cans are assumed to have a variance of 4 grams, what does the
average weight need to be in order to ensure that the 99% of all cans
have a weight of at least 250 grams?</li>
</ul>
<p>The normal distribution with parameters <span class="math">\(\mu\)</span> and <span class="math">\(\sigma\)</span>
is denoted as <span class="math">\(N(\mu,\sigma)\)</span>. If the <em>random variate (rv)</em> <em>X</em> is
normally distributed with expectation <span class="math">\(\mu\)</span> and standard deviation
<span class="math">\(\sigma\)</span>, one denotes: <span class="math">\(\,X \sim N(\mu,\sigma)\)</span> or
<span class="math">\(\,X \in N(\mu,\sigma)\)</span>.</p>
<table border="1" class="docutils">
<colgroup>
<col width="32%" />
<col width="33%" />
<col width="35%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">Range</th>
<th class="head">Prob. of being within</th>
<th class="head">Prob. of being outside</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>mean <span class="math">\(\pm\)</span> 1SD</td>
<td>0.683</td>
<td>0.317</td>
</tr>
<tr class="row-odd"><td>mean <span class="math">\(\pm\)</span> 2SD</td>
<td>0.954</td>
<td>0.046</td>
</tr>
<tr class="row-even"><td>mean <span class="math">\(\pm\)</span> 3SD</td>
<td>0.9973</td>
<td>0.0027</td>
</tr>
</tbody>
</table>
<p>The Figure below shows a number of functions are
commonly used to select appropriate points from the normal distribution
function:</p>
<ul class="simple">
<li><em>Probability density function (PDF)</em>: note that to obtain the
probability for the variable appearing in a certain interval, you
have to <em>integrate</em> the PDF over that range.</li>
<li><em>Cumulative distribution function (CDF)</em>: gives the probability of
obtaining a value smaller than the given value</li>
<li><em>Survival function (SF)</em>: 1-CDF</li>
<li><em>Percentile point function (PPF)</em>: the inverse of the CDF. Answers
the question &#8220;Given a certain probability, what is the corresponding
value for the CDF?&#8221;</li>
<li><em>Inverse survival function (ISF)</em>: the name says it all.</li>
</ul>
<div class="line-block">
<div class="line"><a class="reference internal" href="_images/DistributionFunctions.png"><img alt="image10" src="_images/DistributionFunctions.png" style="width: 611.25px; height: 461.25px;" /></a></div>
</div>
<p><em>Utility functions for continuous distributions, here for the normal distribution.</em></p>
</div>
<div class="section" id="other-continuous-distributions">
<h2>Other Continuous Distributions<a class="headerlink" href="#other-continuous-distributions" title="Permalink to this headline">¶</a></h2>
<div class="section" id="t-distribution">
<h3>t Distribution<a class="headerlink" href="#t-distribution" title="Permalink to this headline">¶</a></h3>
<p>For a small number of samples (ca.:math:<cite>&lt;10</cite>) from a normal
distribution, the distribution of the mean deviates slightly from the
normal distribution. The reason is that the sample mean does not
coincide exactly with the population mean. This modified distribution is
the <em>t-distribution</em>, and converges for larger values towards the normal
distribution (Fig. [fig:t]).</p>
<div class="line-block">
<div class="line"><a class="reference internal" href="_images/Student_t_pdf.png"><img alt="image11" src="_images/Student_t_pdf.png" style="width: 400.0px; height: 320.0px;" /></a></div>
</div>
<p><em>t Distribution</em></p>
</div>
<div class="section" id="chi-square-distribution">
<h3>Chi-square Distribution<a class="headerlink" href="#chi-square-distribution" title="Permalink to this headline">¶</a></h3>
<p>The <em>Chi-square distribution</em> is related to normal distribution in a
simple way: If a random variable <span class="math">\(X\)</span> has a normal distribution
(<span class="math">\(X \in N(0,1)\)</span>), then <span class="math">\(X^2\)</span> has a chi-square distribution,
with one degree of freedom (<span class="math">\(X^2 \in \chi_{1}^2\)</span>). The sum squares
of <span class="math">\(n\)</span> independent and standard normal random variables has a
chi-square distribution with <span class="math">\(n\)</span> degrees of freedom:</p>
<div class="math">
\[\sum\limits_{i = 1}^n {X_i^2} \in \chi_{n}^2\]</div>
<div class="line-block">
<div class="line"><a class="reference internal" href="_images/ChiSquare_pdf.png"><img alt="image12" src="_images/ChiSquare_pdf.png" style="width: 400.0px; height: 266.8px;" /></a></div>
</div>
<p><em>Chi-square Distribution</em></p>
</div>
<div class="section" id="f-distribution">
<h3>F Distribution<a class="headerlink" href="#f-distribution" title="Permalink to this headline">¶</a></h3>
<p>Named after Sir Ronald Fisher, who developed the F distribution for use
in determining critical values in ANOVAs (<em>ANalysis Of VAriance</em>). The
cutoff values in an F table are found using three variables:</p>
<ul class="simple">
<li>ANOVA numerator degrees of freedom</li>
<li>ANOVA denominator degrees of freedom</li>
<li>significance level</li>
</ul>
<p>ANOVA compares the size of the variance between two different samples.
This is done by dividing the larger variance over the smaller variance.
The formula for the resulting <em>F statistic</em> is:</p>
<div class="math">
\[F(r_1, r_2) = \frac{\chi_{r1} ^2 /r_1}{\chi_{r2} ^2 /r_2}\]</div>
<p>where <span class="math">\(\chi_{r1}^2\)</span> and <span class="math">\(\chi_{r2}^2\)</span> are the chi-square
statistics of sample one and two respectively, and <span class="math">\(r_1\)</span> and
<span class="math">\(r_2\)</span> are their degrees of freedom, i.e. the number of
observations.</p>
<div class="section" id="f-test-of-equality-of-variances">
<h4>F-Test of Equality of Variances<a class="headerlink" href="#f-test-of-equality-of-variances" title="Permalink to this headline">¶</a></h4>
<p>One example could be if you want to compare apples that look alike but
are from different trees and have different sizes. If you want to
investigate whether they have the same variance of the weight on
average, you have to calculate</p>
<div class="math">
\[F = \frac{S_x^2}{S_y^2}\]</div>
<p>where <span class="math">\(S_x\)</span> ist he sample standard deviation of the first batch of
apples, and <span class="math">\(S_y\)</span> the sample standard deviation for the second
batch of apples.</p>
<p>There are three apples from the first tree that weigh 110, 121 and 143
grams respectively, and four from the other which weigh 88, 93, 105 and
124 grams respectively. The F statistic is <span class="math">\(F 1.05\)</span>, and has
<span class="math">\(n-1\)</span> and <span class="math">\(m-1\)</span> degrees of freedom, where <span class="math">\(n\)</span> and
<span class="math">\(m\)</span> are the number of apples in each batch. The code sample below
shots what the F statistic is close to the center of the distribution,
so we cannot reject the hypthesis that the two batches have the same
variance.</p>
<div class="highlight-python"><pre>In [1]:  apples1 = array([110, 121, 143])
In [2]:  apples2 = array([88, 93, 105, 124])
In [3]:  fval = std(apples1, ddof=1)/std(apples2, ddof=1)
In [4]:  fd = stats.distributions.f(3,4)
In [5]:fd.cdf(fval)
Out[27]: 0.537640478466751</pre>
</div>
<div class="line-block">
<div class="line"><a class="reference internal" href="_images/F_distributionPDF.png"><img alt="image13" src="_images/F_distributionPDF.png" style="width: 409.6px; height: 307.2px;" /></a></div>
</div>
<p><em>F Distribution</em></p>
</div>
</div>
<div class="section" id="lognormal-distribution">
<h3>Lognormal Distribution<a class="headerlink" href="#lognormal-distribution" title="Permalink to this headline">¶</a></h3>
<p>In some circumstances a set of data with a positively skewed
distribution can be transformed into a symmetric distribution by taking
logarithms. Taking logs of data with a skewed distribution will often
give a distribution that is near to normal (see Figure [fig:lognormal]).</p>
<div class="line-block">
<div class="line"><a class="reference internal" href="_images/LogNormal_Linear.png"><img alt="image14" src="_images/LogNormal_Linear.png" style="width: 326.0px; height: 246.0px;" /></a></div>
</div>
<p><em>Plotted against a linear abscissa.</em></p>
<div class="line-block">
<div class="line"><a class="reference internal" href="_images/LogNormal_Logarithmic.png"><img alt="image15" src="_images/LogNormal_Logarithmic.png" style="width: 326.0px; height: 246.0px;" /></a></div>
</div>
<p>Plotted against a logarithmic abscissa.
<em>Plotted against a logarithmic abscissa.</em></p>
</div>
<div class="section" id="exponential-distribution">
<h3>Exponential Distribution<a class="headerlink" href="#exponential-distribution" title="Permalink to this headline">¶</a></h3>
<p>For a stochastic variable X with an <em>exponential distribution</em>, the
probability distribution function is:</p>
<div class="math">
\[\begin{split}\label{eq_exponential}
f_x (x) =
  \begin{cases}
\lambda e^{- \lambda x}, &amp; \mbox{if } x \ge 0 \\
0, &amp; \mbox{if } x &lt; 0
\end{cases}\end{split}\]</div>
<p>The exponential PDF is shown in Figure [fig:exponential]</p>
<div class="line-block">
<div class="line"><a class="reference internal" href="_images/Exponential_pdf.png"><img alt="image16" src="_images/Exponential_pdf.png" style="width: 400.0px; height: 320.0px;" /></a></div>
</div>
<p><em>Exponential Distribution</em></p>
</div>
<div class="section" id="uniform-distribution">
<h3>Uniform Distribution<a class="headerlink" href="#uniform-distribution" title="Permalink to this headline">¶</a></h3>
<p>This is a simple one: an even probability for all data values (Figure
[fig:uniform]). Not very common for real data.</p>
<div class="line-block">
<div class="line"><a class="reference internal" href="../Images/Uniform_Distribution_PDF.png"><img alt="image17" src="../Images/Uniform_Distribution_PDF.png" /></a></div>
</div>
<p><em>Uniform Distribution</em></p>
</div>
<div class="section" id="programs-continuous-distribution-functions">
<h3>Programs: Continuous Distribution Functions<a class="headerlink" href="#programs-continuous-distribution-functions" title="Permalink to this headline">¶</a></h3>
<p>See also the ipython notebook <a class="reference external" href="http://nbviewer.ipython.org/url/work.thaslwanter.at/CSS/Code/dist_continuous.ipynb">dist_continuous.ipynb</a>:</p>
</div>
</div>
<div class="section" id="discrete-distributions">
<h2>Discrete Distributions<a class="headerlink" href="#discrete-distributions" title="Permalink to this headline">¶</a></h2>
<p>While the functions describing continuous distributions are referred to
as <em>probability distribution functions</em>, discrete distributions are
described by <em>probability mass functions</em>.</p>
<div class="section" id="binomial-distribution">
<h3>Binomial Distribution<a class="headerlink" href="#binomial-distribution" title="Permalink to this headline">¶</a></h3>
<p>The Binomial is associated with the question &#8220;Out of a given number of
trials, how many will succeed?&#8221; Some example questions that are modeled
with a Binomial distribution are:</p>
<ul class="simple">
<li>Out of ten tosses, how many times will this coin land ”heads”?</li>
<li>From the children born in a given hospital on a given day, how many
of them will be girls?</li>
<li>How many students in a given classroom will have green eyes?</li>
<li>How many mosquitos, out of a swarm, will die when sprayed with
insecticide?</li>
</ul>
<p>We conduct <span class="math">\(n\)</span> repeated experiments where the probability of
success is given by the parameter <span class="math">\(p\)</span> and add up the number of
successes. This number of successes is represented by the random
variable <span class="math">\(X\)</span>. The value of <span class="math">\(X\)</span> is then between 0 and
<span class="math">\(n\)</span>.</p>
<p>When a random variable X has a Binomial Distribution with parameters
<span class="math">\(p\)</span> and <span class="math">\(n\)</span> we write it as <span class="math">\(\,X \sim Bin(n,p)\)</span> or
<span class="math">\(\,X \sim B(n,p)\)</span> and the probability mass function is given at
<span class="math">\(X=k\)</span> by the equation:</p>
<div class="math">
\[\begin{split}P\left[X = k\right] = \begin{cases} {n \choose k} p^k \left(1-p\right)^{n-k}\ &amp; 0 \le k \le n \\ 0 &amp; \mbox{otherwise} \end{cases} \quad 0 \leq p \leq 1, \quad n \in \mathbb{N}\end{split}\]</div>
<p>where <span class="math">\({n \choose k}={n! \over k!(n-k)!}\)</span></p>
<div class="line-block">
<div class="line"><a class="reference internal" href="_images/Binomial_distribution_pmf.png"><img alt="image18" src="_images/Binomial_distribution_pmf.png" style="width: 400.0px; height: 266.4px;" /></a></div>
</div>
<p><em>Binomial Distribution</em></p>
</div>
<div class="section" id="poisson-distribution">
<h3>Poisson Distribution<a class="headerlink" href="#poisson-distribution" title="Permalink to this headline">¶</a></h3>
<p>Any French speaker will notice that &#8220;Poisson&#8221; means &#8220;fish&#8221;, but really
there’s nothing fishy about this distribution. It’s actually pretty
straightforward. The name comes from the mathematician Siméon-Denis
Poisson (1781-1840).</p>
<p>The Poisson Distribution is ”very similar” to the Binomial Distribution.
We are examining the number of times an event happens. The difference is
subtle. Whereas the Binomial Distribution looks at how many times we
register a success over a fixed total number of trials, the Poisson
Distribution measures how many times a discrete event occurs, over a
period of continuous space or time. There isn’t a &#8220;total&#8221; value n. As
with the previous sections, let’s examine a couple of experiments or
questions that might have an underlying Poisson nature.</p>
<ul class="simple">
<li>How many pennies will I encounter on my walk home?</li>
<li>How many children will be delivered at the hospital today?</li>
<li>How many products will I sell after airing a new television
commercial?</li>
<li>How many mosquito bites did you get today after having sprayed with
insecticide?</li>
<li>How many defects will there be per 100 metres of rope sold?</li>
</ul>
<p>What’s a little different about this distribution is that the random
variable <span class="math">\(X\)</span> which counts the number of events can take on <em>any
non-negative integer</em> value. In other words, I could walk home and find
no pennies on the street. I could also find one penny. It’s also
possible (although unlikely, short of an armored-car exploding nearby)
that I would find 10 or 100 or 10,000 pennies.</p>
<p>Instead of having a parameter p that represents a component probability
like in the Binomial distribution, this time we have the parameter
&#8220;lambda&#8221; or <span class="math">\(\lambda\)</span> which represents the &#8220;average or expected&#8221;
number of events to happen within our experiment. The probability mass
function of the Poisson is given by</p>
<div class="math">
\[P(X=k)=\frac{e^{-\lambda}\lambda^k}{k!}\]</div>
<p>.</p>
<div class="line-block">
<div class="line"><a class="reference internal" href="_images/Poisson_pmf.png"><img alt="image19" src="_images/Poisson_pmf.png" style="width: 400.0px; height: 320.0px;" /></a></div>
</div>
<p><em>Poisson Distribution</em></p>
</div>
<div class="section" id="programs-discrete-distribution-functions">
<h3>Programs: Discrete Distribution Functions<a class="headerlink" href="#programs-discrete-distribution-functions" title="Permalink to this headline">¶</a></h3>
<p>See also the ipython notebook <a class="reference external" href="http://nbviewer.ipython.org/url/work.thaslwanter.at/Stats/ipynb/dist_discrete.ipynb">dist_discrete.ipynb</a>:</p>
</div>
</div>
</div>
<div class="section" id="data-analysis">
<h1>Data Analysis<a class="headerlink" href="#data-analysis" title="Permalink to this headline">¶</a></h1>
<div class="section" id="data-screening">
<h2>Data Screening<a class="headerlink" href="#data-screening" title="Permalink to this headline">¶</a></h2>
<p>The first thing you have to do for your data analysis is simply <em>look at
your data</em>. You should check for <em>missing data</em> in your data set, and
<em>outliers</em> which can significantly influence the result of your
analysis.</p>
</div>
<div class="section" id="normality-check">
<h2>Normality Check<a class="headerlink" href="#normality-check" title="Permalink to this headline">¶</a></h2>
<p>The first way to check if your data are normally distributed, i.e. that
they are linearly related to the standard normal distribution. In
statistics, <em>:math:`Q–Q` plots</em> (&#8220;Q&#8221; stands for quantile) are used for
visual assessments of distributions. They are a graphical method for
comparing two probability distributions by plotting their quantiles
against each other. First, the set of intervals for the quantiles are
chosen. A point <span class="math">\((x,y)\)</span> on the plot corresponds to one of the
quantiles of the second distribution (y-coordinate) plotted against the
same quantile of the first distribution (x-coordinate). Thus the line is
a parametric curve with the parameter which is the (number of the)
interval for the quantile.</p>
<p>If the two distributions being compared are similar, the points in the
<span class="math">\(Q-Q\)</span> plot will approximately lie on the line <span class="math">\(y = x\)</span>. If
the distributions are linearly related, the points in the <span class="math">\(Q-Q\)</span>
plot will approximately lie on a line, but not necessarily on the line
<span class="math">\(y = x\)</span> (Figure [fig:qqplot]).</p>
<div class="line-block">
<div class="line"><a class="reference internal" href="_images/ProbPlot.png"><img alt="image20" src="_images/ProbPlot.png" style="width: 407.5px; height: 307.5px;" /></a></div>
</div>
<p><em>QQ-plot</em></p>
<p>In addition, there are quantitative tests for normality. The test that I
have encountered most frequently in recent literature is the
<em>Kolmogorov-Smirnov test</em>.  <a href="#id4"><span class="problematic" id="id5"><span id="id1"></span>[1]_</span></a> Altman mainly uses the <em>Shapiro-Wilk W
test</em> , and a number of other tests are also available.</p>
</div>
<div class="section" id="transformation">
<h2>Transformation<a class="headerlink" href="#transformation" title="Permalink to this headline">¶</a></h2>
<p>If your data deviate significantly from a normal distribution, it is
sometimes possible to make the distribution approximately normal by
transforming your data. For example, data often have values that can
only be positive (e.g. the size of persons), and that have long positive
tail: such data can often be made normal by applying a <em>log transform</em>.
This is demonstrated in Figure [fig:lognormal].</p>
</div>
<div class="section" id="confidence-intervals">
<h2>Confidence Intervals<a class="headerlink" href="#confidence-intervals" title="Permalink to this headline">¶</a></h2>
<p>Although it is common to concentrate the analysis on the p-values, it is
often much more informative to report the <em>confidence intervals</em> for
your data. The confidence intervals are given by</p>
<div class="math">
\[ci = mean \pm se * t_{n,\alpha}\]</div>
<p>where <span class="math">\(se\)</span> is the standard error, and <span class="math">\(t_{n,\alpha}\)</span> the
<span class="math">\(t\)</span> statistic for <span class="math">\(n\)</span> degrees of freedom. For the 95%
two-sided confidence intervals, for example, you have to set
<span class="math">\(\alpha=0.025\)</span> and <span class="math">\(\alpha=0.975\)</span> .</p>
</div>
</div>


          </div>
        </div>
      </div>
      <div class="sphinxsidebar">
        <div class="sphinxsidebarwrapper">
  <h3><a href="StatsFH.html">Table Of Contents</a></h3>
  <ul>
<li><a class="reference internal" href="#">Characterizing a Distribution</a><ul>
<li><a class="reference internal" href="#distribution-center">Distribution Center</a><ul>
<li><a class="reference internal" href="#mean">Mean</a></li>
<li><a class="reference internal" href="#median">Median</a></li>
<li><a class="reference internal" href="#mode">Mode</a></li>
<li><a class="reference internal" href="#geometric-mean">Geometric Mean</a></li>
</ul>
</li>
<li><a class="reference internal" href="#quantifying-variability">Quantifying Variability</a><ul>
<li><a class="reference internal" href="#range">Range</a></li>
<li><a class="reference internal" href="#centiles">Centiles</a></li>
<li><a class="reference internal" href="#standard-deviation-and-variance">Standard Deviation and Variance</a></li>
<li><a class="reference internal" href="#standard-error">Standard Error</a></li>
<li><a class="reference internal" href="#skewness">Skewness</a></li>
<li><a class="reference internal" href="#central-limit-theorem">Central Limit Theorem</a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#distribution-functions">Distribution Functions</a><ul>
<li><a class="reference internal" href="#probability-and-samples">Probability and Samples</a></li>
<li><a class="reference internal" href="#normal-distribution">Normal Distribution</a></li>
<li><a class="reference internal" href="#other-continuous-distributions">Other Continuous Distributions</a><ul>
<li><a class="reference internal" href="#t-distribution">t Distribution</a></li>
<li><a class="reference internal" href="#chi-square-distribution">Chi-square Distribution</a></li>
<li><a class="reference internal" href="#f-distribution">F Distribution</a><ul>
<li><a class="reference internal" href="#f-test-of-equality-of-variances">F-Test of Equality of Variances</a></li>
</ul>
</li>
<li><a class="reference internal" href="#lognormal-distribution">Lognormal Distribution</a></li>
<li><a class="reference internal" href="#exponential-distribution">Exponential Distribution</a></li>
<li><a class="reference internal" href="#uniform-distribution">Uniform Distribution</a></li>
<li><a class="reference internal" href="#programs-continuous-distribution-functions">Programs: Continuous Distribution Functions</a></li>
</ul>
</li>
<li><a class="reference internal" href="#discrete-distributions">Discrete Distributions</a><ul>
<li><a class="reference internal" href="#binomial-distribution">Binomial Distribution</a></li>
<li><a class="reference internal" href="#poisson-distribution">Poisson Distribution</a></li>
<li><a class="reference internal" href="#programs-discrete-distribution-functions">Programs: Discrete Distribution Functions</a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#data-analysis">Data Analysis</a><ul>
<li><a class="reference internal" href="#data-screening">Data Screening</a></li>
<li><a class="reference internal" href="#normality-check">Normality Check</a></li>
<li><a class="reference internal" href="#transformation">Transformation</a></li>
<li><a class="reference internal" href="#confidence-intervals">Confidence Intervals</a></li>
</ul>
</li>
</ul>

  <h4>Previous topic</h4>
  <p class="topless"><a href="statsBasics.html"
                        title="previous chapter">Datatypes</a></p>
  <h4>Next topic</h4>
  <p class="topless"><a href="statsTests.html"
                        title="next chapter">Hypothesis tests</a></p>
  <h3>This Page</h3>
  <ul class="this-page-menu">
    <li><a href="_sources/statsDistributions.txt"
           rel="nofollow">Show Source</a></li>
  </ul>
<div id="searchbox" style="display: none">
  <h3>Quick search</h3>
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" />
      <input type="submit" value="Go" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    <p class="searchtip" style="font-size: 90%">
    Enter search terms or a module, class or function name.
    </p>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="statsTests.html" title="Hypothesis tests"
             >next</a> |</li>
        <li class="right" >
          <a href="statsBasics.html" title="Datatypes"
             >previous</a> |</li>
        <li><a href="StatsFH.html">Introduction to Statistics 1.0 documentation</a> &raquo;</li> 
      </ul>
    </div>
    <div class="footer">
        &copy; Copyright March 2013, Thomas Haslwanter.
      Created using <a href="http://sphinx.pocoo.org/">Sphinx</a> 1.1.2.
    </div>
  </body>
</html>